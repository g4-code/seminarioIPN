{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Science\n",
    "#Coder: Marco Galicia\n",
    "#Alma mater : IPN México\n",
    "#\"Yo solo sé que no se nada\"...olvida todas las palabras griegas y los rollos de papiro...\n",
    "import numpy as np # linear algebra\n",
    "from numpy import array\n",
    "#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1_ML<br>\n",
    "Se propone una linea recta para aplicar regresion estadistica sobre un grupo de datos dispersos.\n",
    "\n",
    "La dispersion de los datos es conocida(least squares)\n",
    "Se incluye la funcion para least squares, otra para observar el error y poder corregir.\n",
    "Graficas de dispersion incuidas.\n",
    "\n",
    "@online{classicStatistics,\n",
    "  title=\"Probability and Statistics\",\n",
    "  author=\"A.M.Mathai, P.N. Rathie\",\n",
    "  year=\"1977\",\n",
    "  doi=\"https://doi.org/10.1007/978-1-349-02767-5\",\n",
    "  publisher=\"The MacMillan press LTD.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Refactor\n",
    "#C1_ML\n",
    "#estadistica clasica: asumiendo una dispersion bien conocida, \n",
    "#x_test=array([0,1,-1,2])\n",
    "#y_test=array([3,4,2,8])\n",
    "#N numero de muestras...puntos p(x,y)\n",
    "#N_test = x_test.size\n",
    "#funcion: ecuaciones leats square para curva linear estima y^\n",
    "def leastSquareCustom(x,y,N, inclinacion):\n",
    "    if(inclinacion == 0):\n",
    "        inclinacion=0\n",
    "    #b^ = b_weigth\n",
    "    #secuencia 1\n",
    "    sumatoria_sec1 = 0\n",
    "    for i in range(N):\n",
    "        sumatoria_sec1 = sumatoria_sec1 + (x[i]*y[i])\n",
    "\n",
    "    b_sec1 = (sumatoria_sec1/N)\n",
    "    print(\"b_sec1 :\",b_sec1)\n",
    "    #b secuencia 2, mean de y en realidad es frac algebraica\n",
    "    b_sec2 = ((np.mean(x))*(np.mean(y)))\n",
    "    print(\"b_sec2 :\",b_sec2)\n",
    "    #b secuencia 3\n",
    "    b_sec3 = b_sec1 - b_sec2\n",
    "\n",
    "    print(\"b_sec3 :\",b_sec3)\n",
    "    #b secuencia 4\n",
    "    sumatoria_sec4 = 0\n",
    "    for i in range(N):\n",
    "        sumatoria_sec4 = sumatoria_sec4 + pow(x[i],2)\n",
    "    b_sec4 = (sumatoria_sec4/N)\n",
    "    #b secuencia 5\n",
    "    b_sec5 = pow(np.mean(x),2)\n",
    "    #b secuencia 6\n",
    "    b_sec6 = b_sec4 - b_sec5\n",
    "    print(\"b_sec6 :\",b_sec6)\n",
    "    #b secuencia 7: experimentos con angulo de inclnacion para reducir error (fine tunning, hyperparams bla bla bla)\n",
    "    b_weigth = (b_sec3/b_sec6) - inclinacion\n",
    "    b_weigth_original = (b_sec3/b_sec6)\n",
    "    #b secuencia 7: original\n",
    "    #b_weigth = b_sec3/b_sec6\n",
    "    print(\"b_weigth inclinado:\",b_weigth)\n",
    "    print(\"b_weigth original:\",b_weigth_original)\n",
    "    #2 secuencias para a^= a_bias, x mean en realidad es frac algebraica\n",
    "    a_sec1 = np.mean(y)\n",
    "    #print(\"a_sec1 :\",a_sec1)\n",
    "    #a secuencia 2\n",
    "    a_sec2 = (b_weigth_original)*(np.mean(x))\n",
    "    #print(\"a_sec2 :\",a_sec2)\n",
    "    a_bias = a_sec1 - a_sec2\n",
    "    print(\"a_bias :\",a_bias)\n",
    "    #se procede a calcular las ecuaciones de ^y estimate\n",
    "    y_estimate = []\n",
    "    for i in range(N):\n",
    "        #print(a_bias)\n",
    "        #print(x[i])\n",
    "        #print(b_weigth)\n",
    "        #print(b_weigth*x[i])\n",
    "        ecSecuence1 = 0\n",
    "        ecuacion = 0\n",
    "        ecSecuence1 = b_weigth*x[i]\n",
    "        ecuacion = round(a_bias + ecSecuence1,5)\n",
    "        #print(a_bias - (b_weigth)*(x[i]))\n",
    "        y_estimate.append(ecuacion)\n",
    "    \n",
    "    print(\"y:\",*y)\n",
    "    print(\"y_estimate:\",*y_estimate)\n",
    "    print(\"y_estimate_len:\",len(y_estimate))\n",
    "    return y_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.3.1\n",
    "#x_c=array([0,1,2])\n",
    "#y_c=array([1,5,7])\n",
    "#11.31.1\n",
    "#x_c=array([0,1,2])\n",
    "#y_c=array([4,7,12])\n",
    "#ejemplo custom\n",
    "x_c=array([2,1,4,-1,3,-3,-6,5])\n",
    "y_c=array([1,3,4,1,8,-3,-2,7])\n",
    "inclinacion = 0\n",
    "#N numero de muestras...puntos p(x,y)\n",
    "N_test = x_c.size\n",
    "######## dispersion original\n",
    "fig, ax_disp = plt.subplots()\n",
    "#estimator scatterplot\n",
    "ax_disp.scatter(x_c, y_c)\n",
    "plt.show()\n",
    "y_c_est_exp1 = leastSquareCustom(x_c,y_c,N_test,inclinacion)\n",
    "###### hyperplano: despues de leats squares\n",
    "fig, ax_estimado = plt.subplots()\n",
    "#estimator scatterplot\n",
    "ax_estimado.scatter(x_c, y_c, c='g', marker=\"p\", label='Dispersion original')\n",
    "ax_estimado.scatter(x_c, y_c_est_exp1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gráfica de arriba se muestra la dispersión original en verde, en azul la regresión linear, despues de calcular least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meh!\n",
    "#desviacion estandar sigma\n",
    "#Varianza secuencia 1\n",
    "def sigmaCustom(y_estimate_rel,length):\n",
    "    y_estVarSec1 = 0\n",
    "    #Nv = len(y_est)\n",
    "    for i in range(length):\n",
    "         y_estVarSec1 = y_estVarSec1 + y_estimate_rel[i]\n",
    "    #print(y_estVarSec1)\n",
    "    #varianza sec 2\n",
    "    y_estVarSec2 = 0\n",
    "    for i in range(length):\n",
    "         y_estVarSec2 = y_estVarSec2 + pow(y_estimate_rel[i],2)\n",
    "    #print(y_estVarSec2)\n",
    "    #varianza relativa\n",
    "    varianzaRel = (y_estVarSec2/length) - (pow(y_estVarSec1,2)/pow(length,2))\n",
    "    print(\"varianza_Relativa:\",varianzaRel)\n",
    "    #sigma: desviacion estandar relativa (desde mean y_est...^y)\n",
    "    #sigma: error standar / data polling\n",
    "    sigmaRel = np.sqrt(varianzaRel)\n",
    "    print(\"sigma_Relativa (Error estandar):\",sigmaRel)\n",
    "#dcheck M.S.R.E. algorithm\n",
    "sigmaCustom(y_c_est_exp1,N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1_ML\n",
    "Test for reaching ≧ 68% accuracy (normal distribution rules)\n",
    "\n",
    "Utilizando el algoritmo de ref, la RSD ≈ 3.149 ; para N = 8 ; 39.3% de error (60.7% de probabilidad). El modelo de linea no realiza un buen ajuste desde el punto de vista SD,\n",
    "\n",
    "Pero algunos puntos de prediccion son bastante cercanos a los puntos de la dispersion original.\n",
    "\n",
    "**Exp 1 hyperparams & fine tunning.**\n",
    "Se quiere modificar el angulo de inclinacion de la pendiente ordenada al origen para la recta. La intencion es colocar la linea en diferentes inclinaciones hasta mejorar la STD, sin salir de los rangos.(dcheck rangos...hay un error.)\n",
    "\n",
    "Verificando m\n",
    "m=Θ=b^\n",
    "\n",
    "No confundir b^ con bias, en la ecuacion de la recta, b^ es weigth, y a^ es bias (lo relativo a y), se puede comprobar lo relativo graficando en un papel cuadrado, por simple inspeccion,\n",
    "pasa por el punto p(0,b) ≈ (0, 1.8121), la pendiente de la recta corta en a^≈ 1.8121.\n",
    "\n",
    "Con b^ modificamos el angulo de inclinacion de m,\n",
    "terminologia g.analitica m=tgΘ en y=mx + b ; p(0,b) ... \n",
    "terminologia least squares y^ = ^a + ^bx; p(0, ^a)\n",
    "terminologia machine learning y^ = ^bias + ^wx; p(0, ^bias)\n",
    "\n",
    "Comprobacion de valores:\n",
    "least squares\n",
    "m=tgΘ = y2 - y1/x2 - x1 = (6.31)-(-3.59)/(5)-(-6) ≈ 0.9 \n",
    "y se sabe que b_weigth ≈ 0.9003831417624522\n",
    "\n",
    "exp1\n",
    "m=tgΘ = y2 - y1/x2 - x1 = (5.38519)-(-2.18329)/(5)-(-6) ≈ 0.68 y se sabe que b_weigth ≈ 0.68\n",
    "\n",
    "En y = mx + b ; m(0,b); y^ = ^a + ^bx; ^b(0, ^a)\n",
    "\n",
    "Si se mueve el angulo de inclinacion Θ de la recta, puede que se alcance una mejor RSD.\n",
    "Se propone mutacion en b^.\n",
    "\n",
    "Despues del Exp1, se paso a la funcion customLeast un parametro de ajuste para el angulo de inclinacion de la pendiente, resultados:\n",
    "\n",
    "inclinacion  0.21234 -> RSD ≈ 2.406\n",
    "\n",
    "El rango observable donde la inclinacion de la recta se mantiene \"cerca\" de los puntos dispersos es (0.2,0.3), mas alla, la inclinacion ya no parece trazar el hyperplano tipo recta sobre la dispersion.\n",
    "\n",
    "Puede que haya mas dispersion entre los puntos, pero hay mejor RSD. Lost Cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inclinacion = 0.21234\n",
    "y_c_est_exp2 = leastSquareCustom(x_c,y_c,N_test,inclinacion)\n",
    "sigmaCustom(y_c_est_exp2,N_test)\n",
    "#https://stackoverflow.com/questions/4270301/matplotlib-multiple-datasets-on-the-same-scatter-plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(x_c, y_c, s=10, c='g', marker=\"p\", label='Dispersion original')\n",
    "ax1.scatter(x_c, y_c_est_exp1, s=10, c='b', marker=\"s\", label='Least squares')\n",
    "ax1.scatter(x_c,y_c_est_exp2, s=10, c='r', marker=\"o\", label='Exp 1 inclinacion')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de pasar un  parametro de Inclinacion (hyperparam),se obtubo una RSD ≈ 2.406, para la curva de recta propuesta(hyperplano).\n",
    "\n",
    "Se logro reducir el error de 39% a 30.075%, este modelo realiza un buen ajuste de la dispersion, con una probabilidad de ≈ 69.925%, de acuerdo a la distribucion gauss ya puede ser tomado con seriedad. Entra en el rango de probabilidad, minimo es 68%.\n",
    "\n",
    "Se mejoro la probabilidad con la medicion RSD, pero los puntos de la dispersion original estan mas dispersos de los puntos aproximados. Loss cost effects.\n",
    "\n",
    "El beneficio de la duda o la duda como calamidad.\n",
    "\n",
    "Conclusion.\n",
    "Es un ajuste experimental, no me convence y_predict para ningun caso RSD = 3.149 o 2.406, el modelo de linea no calcula un ajuste confiable, a menos que se modifique la inclinacion de la recta. \n",
    "\n",
    "Lo mas seguro, es que este patron de linea, se proyecte bien a gran escala.\n",
    "Las 3 graficas, arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejercicio tesina, hipotesis\n",
    "#se agregan dos puntos mas en x u se utiliza el modelo de prediccion, para calcular y\n",
    "#dos puntos extra hardcoded\n",
    "x_c1=array([2,1,4,-1,3,-3,-6,5,6,12])\n",
    "#y_c1=array([1,3,4,1,8,-3,-2,7,7.21,12.61])\n",
    "y_c1=array([3.61,2.71,5.41,0.91,4.51,-0.88,-3.59,6.31,7.21,12.61])\n",
    "######despues\n",
    "fig, ax6 = plt.subplots()\n",
    "#estimator scatterplot\n",
    "ax6.scatter(x_c1, y_c1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencias entre, y original con 2 nuevos puntos de predicción; Y y estimate con 2 nuevos puntos de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando resultados ejercicio tesina:\n",
    "#11.31.1 linear regression\n",
    "inclinacion = 0.0\n",
    "x_tesis=array([2,1,4,-1,3,-3,-6,5,6,12])\n",
    "y_tesis=array([1,3,4,1,8,-3,-2,7,7.21,12.61])\n",
    "#y_tesis=array([3.61,2.71,5.41,0.91,4.51,-0.88,-3.59,6.31,7.21,12.61])\n",
    "#x_31=array([0,1,2])\n",
    "#y_31=array([4,7,12])\n",
    "#N numero de muestras...puntos p(x,y)\n",
    "N_test = x_tesis.size\n",
    "y_est_tesis = leastSquareCustom(x_tesis,y_tesis,N_test,inclinacion)\n",
    "inclinacion = 0.34567\n",
    "y_est_teta = leastSquareCustom(x_tesis,y_tesis,N_test,inclinacion)\n",
    "#N_test = x_31.size\n",
    "#y_est_31_1 = leastSquareCustom(x_31,y_31,N_test,inclinacion)\n",
    "sigmaCustom(y_est_tesis,N_test)\n",
    "sigmaCustom(y_est_teta,N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 15\n",
    "x_limit = np.zeros(N)\n",
    "y_limit = np.zeros(N)\n",
    "dashes = [2, 2, 2, 2]\n",
    "x1_slope = np.linspace(0, 0, N, endpoint=False)\n",
    "x2_slope = np.linspace(-8, 0, N, endpoint=False)\n",
    "y2_slope = np.linspace(-5, 1.8, N, endpoint=False)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "#dibujo de las dispersiones\n",
    "ax1.scatter(x_c, y_c, s=10, c='g', marker=\"p\", label='Dispersion original')\n",
    "ax1.scatter(x_tesis, y_est_tesis, s=10, c='b', marker=\"s\", label='Least squares')\n",
    "ax1.scatter(x_tesis,y_est_teta, s=10, c='r', marker=\"v\", label='Exp 1 inclinacion Θ')\n",
    "#dibujo estatico del eje P(0,b)\n",
    "line1, = ax1.plot(x1_slope, y2_slope, '--', linewidth=2, label='p(0,b)')\n",
    "line1.set_dashes(dashes)\n",
    "\n",
    "line2, = ax1.plot(x2_slope, y + 1.8, '--', linewidth=2, label='b ≈ 1.8')\n",
    "line2.set_dashes(dashes)\n",
    "\n",
    "ax1.legend(loc='lower right')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compara resultados std custom vs numpy std\n",
    "#el metodo custom calcula bien sigma, solo para y predict\n",
    "#numpy ofrece las dos sigmas X y Y, pasando axis=0 (operacion en vertical)\n",
    "np_std_c1 = np.array([[1,3.32106],[3,2.63302],[4,4.69715],[1,1.25693],[8,4.0091],[-3,-0.11916],[-2,-2.18329],[7,5.38519]])\n",
    "#axis = 0, vertical en [x,y]\n",
    "print(np.std(np_std_c1, axis=0, dtype=np.float64))\n",
    "np_std_c2 = np.array([[0,3.66],[1,7.66],[2,11.66]])\n",
    "print(np.std(np_std_c2,axis=0,dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRAFICA estatica p(0,b)\n",
    "import matplotlib.pyplot as plt\n",
    "N = 15\n",
    "x = np.zeros(N)\n",
    "y = np.zeros(N)\n",
    "dashes = [2, 2, 2, 2]\n",
    "x1 = np.linspace(0, 0, N, endpoint=True)\n",
    "x2 = np.linspace(-6, 0, N, endpoint=True)\n",
    "y2 = np.linspace(-3, 1.8, N, endpoint=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(x1, y2, '--', linewidth=2,\n",
    "                 label='p(0,b) -> b ≈ 1.8')\n",
    "line1.set_dashes(dashes)\n",
    "\n",
    "line2, = ax.plot(x2, y + 1.8, '--', linewidth=2,\n",
    "                 label='p(0,b) -> b ≈ 1.8')\n",
    "line2.set_dashes(dashes)\n",
    "\n",
    "plt.xlim([-6, 15])\n",
    "plt.ylim([-3, 15])\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando resultados ejercicios finales:\n",
    "#11.8 linear regression\n",
    "inclinacion = 0.0\n",
    "x_11_8=array([0,1,2,3])\n",
    "y_11_8=array([3,6,8,11])\n",
    "#N numero de muestras...puntos p(x,y)\n",
    "N_test = x_11_8.size\n",
    "y_c_est_exp2 = leastSquareCustom(x_11_8,y_11_8,N_test,inclinacion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
